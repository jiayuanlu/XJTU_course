{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rows=7\n",
    "grid_cols=10\n",
    "wind_power=[0,0,0,1,1,1,2,2,1,0]\n",
    "rand=[-1,0,1]\n",
    "prob = np.array([1/3,1/3,1/3])\n",
    "index = np.random.choice(rand, p = prob)\n",
    "action=[[-1,0],[1,0],[0,-1],[0,1],[-1,-1],[-1,1],[1,-1],[1,1]]\n",
    "action_4=[[-1,0],[1,0],[0,-1],[0,1]]\n",
    "action_9=[[-1,0],[1,0],[0,-1],[0,1],[-1,-1],[-1,1],[1,-1],[1,1],[0,0]]\n",
    "reward_each_step=-1\n",
    "gamma=1      #discount_para\n",
    "epsilon=0.1  #greedy\n",
    "alpha=0.5    #stride\n",
    "field=np.zeros((grid_rows,grid_cols),dtype=int)\n",
    "Q=np.zeros((grid_rows,grid_cols,len(action)),dtype=float)\n",
    "Q_4=np.zeros((grid_rows,grid_cols,len(action_4)),dtype=float)\n",
    "Q_9=np.zeros((grid_rows,grid_cols,len(action_9)),dtype=float)\n",
    "policy={}\n",
    "policy_4={}\n",
    "policy_9={}\n",
    "policy_rows=np.zeros((grid_rows,grid_cols),dtype=int)\n",
    "policy_cols=np.zeros((grid_rows,grid_cols),dtype=int)\n",
    "for i in range(grid_rows):\n",
    "    for j in range(grid_cols):\n",
    "        policy[i,j]=[policy_rows[i,j],policy_cols[i,j]]\n",
    "        p=[]\n",
    "        for k in range(len(action)):\n",
    "            p.append(Q[i,j,k])\n",
    "        policy[i,j]=action[np.argmax(p)]\n",
    "policy_rows=np.zeros((grid_rows,grid_cols),dtype=int)\n",
    "policy_cols=np.zeros((grid_rows,grid_cols),dtype=int)\n",
    "for i in range(grid_rows):\n",
    "    for j in range(grid_cols):\n",
    "        policy_4[i,j]=[policy_rows[i,j],policy_cols[i,j]]\n",
    "        p=[]\n",
    "        for k in range(len(action_4)):\n",
    "            p.append(Q_4[i,j,k])\n",
    "        policy_4[i,j]=action_4[np.argmax(p)]\n",
    "policy_rows=np.zeros((grid_rows,grid_cols),dtype=int)\n",
    "policy_cols=np.zeros((grid_rows,grid_cols),dtype=int)\n",
    "for i in range(grid_rows):\n",
    "    for j in range(grid_cols):\n",
    "        policy_9[i,j]=[policy_rows[i,j],policy_cols[i,j]]\n",
    "        p=[]\n",
    "        for k in range(len(action_9)):\n",
    "            p.append(Q_9[i,j,k])\n",
    "        policy_9[i,j]=action_9[np.argmax(p)]\n",
    "episode=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_state(s,act):\n",
    "    position_col=s[1]\n",
    "    next_s_row=s[0]-act[1]-(wind_power[position_col]+rand[index])\n",
    "    next_s_col=s[1]+act[0]\n",
    "    if next_s_col<0:\n",
    "        next_s_col=0\n",
    "    if next_s_col>=grid_cols:\n",
    "        next_s_col=grid_cols-1\n",
    "    if next_s_row<0:\n",
    "        next_s_row=0\n",
    "    if next_s_row>=grid_rows:\n",
    "        next_s_row=grid_rows-1\n",
    "    return next_s_row,next_s_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_8(s):\n",
    "    Q_list=[]\n",
    "    for i in range(len(action)):\n",
    "        Q_list.append(Q[s[0],s[1],i])\n",
    "    A_max=action[np.argmax(Q_list)]\n",
    "    return A_max\n",
    "def epsilon_greedy_4(s):\n",
    "    Q_list=[]\n",
    "    for i in range(len(action_4)):\n",
    "        Q_list.append(Q_4[s[0],s[1],i])\n",
    "    A_max=action_4[np.argmax(Q_list)]\n",
    "    return A_max\n",
    "def epsilon_greedy_9(s):\n",
    "    Q_list=[]\n",
    "    for i in range(len(action_9)):\n",
    "        Q_list.append(Q_9[s[0],s[1],i])\n",
    "    A_max=action_9[np.argmax(Q_list)]\n",
    "    return A_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_trace(T,state):\n",
    "    x=[]\n",
    "    y=[]\n",
    "    plt.figure()\n",
    "    plt.matshow(field)\n",
    "    plt.grid(True)\n",
    "    plt.plot(7,3,markersize=20,color='red',marker='.', linestyle='') \n",
    "    plt.plot(0,3,markersize=20,color='red',marker='.', linestyle='') \n",
    "    for t in range(T):\n",
    "        x.append(state[t][0])\n",
    "        y.append(state[t][1])\n",
    "    plt.plot(np.transpose(y),np.transpose(x))\n",
    "    plt.savefig('trace_Sarsa.png',bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "step_8=[]\n",
    "EP_8=[]\n",
    "step_4=[]\n",
    "EP_4=[]\n",
    "step_9=[]\n",
    "EP_9=[]\n",
    "for i in range(episode):\n",
    "    state={}\n",
    "    t=0\n",
    "    R=0\n",
    "    start_row=3\n",
    "    start_col=0\n",
    "    state[t]=[]\n",
    "    state[t].append(start_row)\n",
    "    state[t].append(start_col)\n",
    "    curr_state=state[t]\n",
    "    c=[]\n",
    "    for j in range (0,len(action)):\n",
    "        c.append(j)\n",
    "    pr=[]  \n",
    "    for k in range(len(action)):\n",
    "        pr.append(1/len(action))\n",
    "    while True:\n",
    "        if np.random.random()<epsilon:\n",
    "            A=action[(np.random.choice(c, p=pr))]\n",
    "        else:\n",
    "            A=epsilon_greedy_8(curr_state)\n",
    "        next_s=next_state(curr_state,A)\n",
    "        R+=reward_each_step\n",
    "        q_max=[]\n",
    "        for m in range(len(action)):\n",
    "            q_max.append(Q[next_s[0],next_s[1],m])\n",
    "        Q[curr_state[0],curr_state[1],action.index(A)]+=alpha*(R+gamma*max(q_max)-Q[curr_state[0],curr_state[1],action.index(A)])\n",
    "        curr_state=next_s\n",
    "        t+=1\n",
    "        state[t]=curr_state\n",
    "        if curr_state[0]==3 and curr_state[1]==7:\n",
    "            break\n",
    "    if i%5==0:\n",
    "        # draw_trace(t+1,state)\n",
    "        step_8.append(t+1)\n",
    "        EP_8.append(i)\n",
    "\n",
    "\n",
    "for i in range(episode):\n",
    "    state={}\n",
    "    t=0\n",
    "    R=0\n",
    "    start_row=3\n",
    "    start_col=0\n",
    "    state[t]=[]\n",
    "    state[t].append(start_row)\n",
    "    state[t].append(start_col)\n",
    "    curr_state=state[t]\n",
    "    c=[]\n",
    "    for j in range (0,len(action_4)):\n",
    "        c.append(j)\n",
    "    pr=[]  \n",
    "    for k in range(len(action_4)):\n",
    "        pr.append(1/len(action_4))\n",
    "    while True:\n",
    "        if np.random.random()<epsilon:\n",
    "            A=action_4[(np.random.choice(c, p=pr))]\n",
    "        else:\n",
    "            A=epsilon_greedy_4(next_s)\n",
    "        next_s=next_state(curr_state,A)\n",
    "        R+=reward_each_step\n",
    "        q_max=[]\n",
    "        for m in range(len(action_4)):\n",
    "            q_max.append(Q_4[next_s[0],next_s[1],m])\n",
    "        Q_4[curr_state[0],curr_state[1],action_4.index(A)]+=alpha*(R+gamma*max(q_max)-Q_4[curr_state[0],curr_state[1],action_4.index(A)])\n",
    "        curr_state=next_s\n",
    "        t+=1\n",
    "        state[t]=curr_state\n",
    "        if curr_state[0]==3 and curr_state[1]==7:\n",
    "            break\n",
    "    if i%5==0:\n",
    "        # draw_trace(t+1,state)\n",
    "        step_4.append(t+1)\n",
    "        EP_4.append(i)\n",
    "\n",
    "\n",
    "for i in range(episode):\n",
    "    state={}\n",
    "    t=0\n",
    "    R=0\n",
    "    start_row=3\n",
    "    start_col=0\n",
    "    state[t]=[]\n",
    "    state[t].append(start_row)\n",
    "    state[t].append(start_col)\n",
    "    curr_state=state[t]\n",
    "    c=[]\n",
    "    for j in range (0,len(action_9)):\n",
    "        c.append(j)\n",
    "    pr=[]  \n",
    "    for k in range(len(action_9)):\n",
    "        pr.append(1/len(action_9))\n",
    "    while True:\n",
    "        if np.random.random()<epsilon:\n",
    "            A=action_9[(np.random.choice(c, p=pr))]\n",
    "        else:\n",
    "            A=epsilon_greedy_9(next_s)\n",
    "        next_s=next_state(curr_state,A)\n",
    "        R+=reward_each_step\n",
    "        q_max=[]\n",
    "        for m in range(len(action_9)):\n",
    "            q_max.append(Q_9[next_s[0],next_s[1],m])\n",
    "        Q_9[curr_state[0],curr_state[1],action_9.index(A)]+=alpha*(R+gamma*max(q_max)-Q_9[curr_state[0],curr_state[1],action_9.index(A)])\n",
    "        curr_state=next_s\n",
    "        t+=1\n",
    "        state[t]=curr_state\n",
    "        if curr_state[0]==3 and curr_state[1]==7:\n",
    "            break\n",
    "    if i%5==0:\n",
    "        # draw_trace(t+1,state)\n",
    "        step_9.append(t+1)\n",
    "        EP_9.append(i)    \n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('episode_num')\n",
    "plt.ylabel('step_num')\n",
    "plt.plot(EP_4,step_4,label=\"4_actions\")\n",
    "plt.plot(EP_8,step_8,label=\"8_actions\")\n",
    "plt.plot(EP_9,step_9,label=\"9_actions\")\n",
    "plt.legend()\n",
    "plt.savefig('trace_QL.png',bbox_inches='tight')\n",
    "plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b4b4feff2f24a0f0a34464dbe537a36fda679851528fb8735cb41fa49dffb2d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
